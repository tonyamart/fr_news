ggplot(aes(x = n, y = numero), fill = word)) + geom_col() +
theme(legend.position = "None")
test2 %>%
separate(mois_2, into = c("numero", "mois")) %>%
group_by(numero) %>%
filter(str_detect(word, "crise*") | str_detect(word, "C|covid*")) %>%
ggplot(aes(x = n, y = numero, fill = word)) + geom_col() +
theme(legend.position = "None")
test2 %>%
separate(mois_2, into = c("numero", "mois")) %>%
mutate(numero = as.numeric(numero)) %>%
group_by(numero) %>%
filter(str_detect(word, "crise*") | str_detect(word, "C|covid*")) %>%
ggplot(aes(x = n, y = numero, fill = word)) + geom_col() +
theme(legend.position = "None")
test2 %>%
# separate(mois_2, into = c("numero", "mois")) %>%
# group_by(numero) %>%
group_by(mois_2) %>%
filter(str_detect(word, "crise*") | str_detect(word, "C|covid*")) %>%
ggplot(aes(x = n, y = numero, fill = word)) + geom_col() +
theme(legend.position = "None")
test2 %>%
# separate(mois_2, into = c("numero", "mois")) %>%
# group_by(numero) %>%
group_by(mois_2) %>%
filter(str_detect(word, "crise*") | str_detect(word, "C|covid*")) %>%
ggplot(aes(x = n, y = mois_2, fill = word)) + geom_col() +
theme(legend.position = "None")
test2 %>%
# separate(mois_2, into = c("numero", "mois")) %>%
# group_by(numero) %>%
group_by(mois_2) %>%
# filter(str_detect(word, "crise*") | str_detect(word, "C|covid*")) %>%
filter(str_detect(word, "crise") | str_detect(word, "C|covid")) %>%
ggplot(aes(x = n, y = mois_2, fill = word)) + geom_col() +
theme(legend.position = "None")
trigrams <- test %>%
group_by(mois) %>%
unnest_tokens(input = text, output = ngram, token = "ngrams", n = 3)
test2 %>%
# separate(mois_2, into = c("numero", "mois")) %>%
# group_by(numero) %>%
group_by(mois_2) %>%
# filter(str_detect(word, "crise*") | str_detect(word, "C|covid*")) %>%
filter(str_detect(word, "crise") | str_detect(word, "C|covid")) %>%
ggplot(aes(x = n, y = mois_2, fill = word)) + geom_col() #+
library(tidyverse)
library(tidytext)
theme_set(theme_minimal())
dat <- read_file(file = "Downloads/LeMonde_aasta_2020.txt")
dat <- tibble(mois = "",
text = dat)
test <- dat %>%
separate_rows(text, sep = "\\*\\*\\*\\* \\*journal_LeMonde ") %>%
mutate(mois = str_extract(text, "\\*mois_\\w+")) %>%
mutate(mois = str_remove_all(mois, "\\*mois_")) %>%
mutate(mois_2 = paste0(row_number(), "_", mois))
str(test)
test2 <- test %>%
group_by(mois_2) %>%
unnest_tokens(input = text, output = word, token = "words") %>%
count(word, sort = T)
glimpse(test2)
library(spacyr)
spacy_initialize(model = "fr_core_web_sm")
library(spacyr)
spacy_initialize(model = "fr_core_news_sm")
# test texts beginnings
test %>%
mutate(mois_test = str_sub(text, start = 1, end = 40)) %>%
select(mois_test)
# test texts beginnings
small <- test %>%
mutate(mois_test = str_sub(text, start = 1, end = 100)) %>%
select(mois_test)
glimpse(small)
head(small)
test %>%
mutate(mois_test = str_sub(text, start = 1, end = 10)) %>%
select(mois_test)
test %>%
mutate(mois_test = str_sub(text, start = 1, end = 40)) %>%
select(mois_test)
test %>%
mutate(mois_test = str_sub(text, start = 1, end = 80)) %>%
select(mois_test)
head(small, 12)
sapply(small$mois_test, spacy_parse(tag = T, lemma = T))
spacy_parse(small$mois_test, post = T, tag = F, lemma = T, entity = T)
slow <- spacy_parce(test$text, pos = T, entity = T, lemma = T, tag = F)
slow <- spacy_parse(test$text, pos = T, entity = T, lemma = T, tag = F)
str(test)
fev <- test %>%
filter(mois == "Février")
fev_[] <- spacy_parse(test$fev, pos = T, entity = T, lemma = T, tag = F)
fev_p <- spacy_parse(fev$text, pos = T, entity = T, lemma = T, tag = F)
fev_p <- spacy_parse(fev$text, pos = T, entity = F, lemma = T, tag = F)
glimpse(fev_p)
fev_p %>%
filter(doc_id == "text2") %>%
head()
fev_p %>%
filter(doc_id == "text2") %>%
head
fev_p %>%
filter(doc_id == "text2") %>%
top_n(10)
t = c("texte textes", "texte textes", "texte textes")
x <- tibble(n = c("f", "m", "a"),
t = c("texte textes", "texte textes", "texte textes"))
x
length(x)
length(x$n)
x <- tibble(n = c("f", "m", "a"),
t = c("texte textes 1", "texte textes 2", "texte textes 3"))
x$t[1]
x$t[2]
for i in 1:length(x$n) {
print(x$t[i])
}
for (i in 1:length(x$n)) {
print(x$t[i])
}
y = NULL
spacy_parse(x$t[i], pos = T, entity = F, lemma = T, tag = F)
y = NULL
for (i in 1:length(x$n)) {
y = spacy_parse(x$t[i], pos = T, entity = F, lemma = T, tag = F)
y
}
head(y)
for (i in 1:length(x$n)) {
y = spacy_parse(x$t[i], pos = T, entity = F, lemma = T, tag = F)
head(y)
}
new_df <- tibble(doc_id = "",
sentence_id = "",
token_id = "",
token = "",
lemma = "",
pos = "")
new_df <- tibble(doc_id = "",
sentence_id = "",
token_id = "",
token = "",
lemma = "",
pos = "",
month = "")
for (i in 1:length(x$n)) {
y = spacy_parse(x$t[i], pos = T, entity = F, lemma = T, tag = F)
y <- y %>%
mutate(month = paste0("loop_", i))
new_df <- rbind(new_df, y)
}
new_df
str(test)
# test texts beginnings
small <- test %>%
mutate(mois_test = str_sub(text, start = 10, end = 100)) %>%
select(mois_test)
small
small -> x
y = NULL
new_df <- tibble(doc_id = "",
sentence_id = "",
token_id = "",
token = "",
lemma = "",
pos = "",
month = "")
for (i in 1:length(x$n)) {
y = spacy_parse(x$t[i], pos = T, entity = F, lemma = T, tag = F)
y <- y %>%
mutate(month = paste0("loop_", i))
new_df <- rbind(new_df, y)
}
x
for (i in 1:length(x$mois_test)) {
y = spacy_parse(x$mois_test[i], pos = T, entity = F, lemma = T, tag = F)
y <- y %>%
mutate(month = paste0("loop_", i))
new_df <- rbind(new_df, y)
}
new_df
View(new_df)
str(test)
x <- test[2,]
str(x)
x <- test[3,]
str(x)
x <- test[:3,]
x <- test[1:3,]
str(x)
y = NULL
new_df <- tibble(doc_id = "",
sentence_id = "",
token_id = "",
token = "",
lemma = "",
pos = "",
month = "")
for (i in 1:length(x$mois_test)) {
y = spacy_parse(x$mois_test[i], pos = T, entity = F, lemma = T, tag = F)
y <- y %>%
mutate(month = paste0("loop_", i))
new_df <- rbind(new_df, y)
}
str(x)
for (i in 1:length(x$text)) {
y = spacy_parse(x$text[i], pos = T, entity = F, lemma = T, tag = F)
y <- y %>%
mutate(month = paste0("loop_", i))
new_df <- rbind(new_df, y)
}
new_df
x <- test
str(x)
y = NULL
new_df <- tibble(doc_id = "",
sentence_id = "",
token_id = "",
token = "",
lemma = "",
pos = "",
month = "")
for (i in 1:length(x$text)) {
y = spacy_parse(x$text[i], pos = T, entity = F, lemma = T, tag = F)
y <- y %>%
mutate(month = paste0("loop_", i))
new_df <- rbind(new_df, y)
}
library(tidyverse)
library(tidytext)
library(openxlsx)
library(stopwords)
library(topicmodels)
theme_set(theme_minimal())
setwd("Documents/ds/scripts/help_fr/")
datt <- read_csv("lemonde_lemmas.csv")
str(datt)
dat <- datt[,-1]
str(dat)
rm(datt)
##### simple topic model #####
word_freq <- dat %>%
unnest_tokens(input = text, output = word, token = "words") %>%
group_by(mois) %>%
count(word, sort = T)
glimpse(word_freq)
# create a matrix of words appearances
dtm <- word_freq %>% cast_dtm(mois, word, n)
month_model <- LDA(dtm, k = 50, control = list(seed = 1234))
# beta -- probabilities of words in topics (sorted, top 10 words selected)
beta <- tidy(month_model, matrix = "beta")
top_words <- beta %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_words)
top_words %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
View(word_freq)
##### stopwords #####
stopwords <- c("le", "de", "à", "être", "avoir", "un", "une",
"et", "qui", "que", "sur", "par", "d", "l", "ce",
"en", "y", "son", "qu", "dans", "c", "pour", "sur",
"il", "plus", "rien", "personne", "tout", "pas", "avec",
"mais", "ne", "n", "s", "se", "faire", "pouvoir", "on")
word_freq <- dat %>%
unnest_tokens(input = text, output = word, token = "words") %>%
filter(!word %in% stopwords) %>%
group_by(mois) %>%
count(word, sort = T)
word_freq <- dat %>%
unnest_tokens(input = text, output = word, token = "words") %>%
group_by(mois) %>%
count(word, sort = T)
word_count <- word_freq %>%
group_by(mois) %>%
summarise(total_n_mois = sum(n))
word_freq <- left_join(word_freq, word_count, by = "mois")
head(word_freq)
word_freq <- word_freq %>%
ungroup() %>%
mutate(perc_mois = n/total_n_mois*100)
month_tf_idf <- bind_tf_idf(word_freq, word, mois, n)
head(month_tf_idf)
tfidf <- month_tf_idf %>%
filter(!word %in% stopwords) %>%
select(-total_n_mois) %>%
slice_max(tf_idf, n = 100) %>%
arrange(desc(tf_idf))
tfidf
write.csv(tfidf, file = "top100_word_montly_tf_idf.csv")
word_freq <- dat %>%
unnest_tokens(input = text, output = word, token = "words") %>%
filter(!word %in% stopwords) %>%
group_by(mois) %>%
count(word, sort = T)
glimpse(word_freq)
stopwords <- c("le", "de", "à", "être", "avoir", "un", "une",
"et", "qui", "que", "sur", "par", "d", "l", "ce",
"en", "y", "son", "qu", "dans", "c", "pour", "sur",
"il", "plus", "rien", "personne", "tout", "pas", "avec",
"mais", "ne", "n", "s", "se", "faire", "pouvoir", "on",
"ou", "aussi", "lui", "comme")
word_freq <- dat %>%
unnest_tokens(input = text, output = word, token = "words") %>%
group_by(mois) %>%
count(word, sort = T)
rm(dtm, month_model)
word_count <- word_freq %>%
group_by(mois) %>%
summarise(total_n_mois = sum(n))
word_freq <- left_join(word_freq, word_count, by = "mois")
head(word_freq)
word_freq <- word_freq %>%
ungroup() %>%
mutate(perc_mois = n/total_n_mois*100)
month_tf_idf <- bind_tf_idf(word_freq, word, mois, n)
head(month_tf_idf)
tfidf <- month_tf_idf %>%
filter(!word %in% stopwords) %>%
select(-total_n_mois) %>%
slice_max(tf_idf, n = 100) %>%
arrange(desc(tf_idf))
write.csv(tfidf, file = "top100_word_montly_tf_idf.csv")
word_freq <- dat %>%
unnest_tokens(input = text, output = word, token = "words") %>%
filter(!word %in% stopwords) %>%
group_by(mois) %>%
count(word, sort = T)
glimpse(word_freq)
# create a matrix of words appearances
dtm <- word_freq %>% cast_dtm(mois, word, n)
dtm
month_model <- LDA(dtm, k = 25, control = list(seed = 1234))
month_model
# beta -- probabilities of words in topics (sorted, top 10 words selected)
beta <- tidy(month_model, matrix = "beta")
top_words <- beta %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_words)
top_words %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
# gamma: probability of topics in documents (months in our case)
gamma <- tidy(month_model, matrix = "gamma")
head(gamma)
gamma %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n))
gamma %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
group_by(month, topic) %>%
summarise(avrg = mean(gamma))
gamma
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma))
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n))
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
ggplot(aes(x = n, y = avrg, fill = topic)) + geom_col()
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
ggplot(aes(x = n, y = avrg, fill = as.factor(topic))) + geom_col()
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
ggplot(aes(x = as.factor(n), y = avrg, fill = as.factor(topic))) + geom_col()
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n))
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
filter(n != 1) %>%
ggplot(aes(x = as.factor(n), y = avrg, fill = as.factor(topic))) + geom_col()
gamma %>%
filter(document = "2_Février")
gamma %>%
filter(document == "2_Février")
top_words %>%
filter(topic == 5)
library(pals)
# distribution of topics over time
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
filter(n != 1) %>%
ggplot(aes(x = as.factor(n), y = avrg, fill = as.factor(topic))) + geom_col() +
scale_fill_manual(values = as.vector(alphabet(n = 25)))
# distribution of topics over time
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
filter(n != 1) %>%
ggplot(aes(x = as.factor(n), y = avrg, fill = as.factor(topic))) + geom_col() +
scale_fill_manual(values = as.vector(alphabet(n = 25))) +
labs(x = "Month",
y = "Distribution of topics",
fill = "Number of topic")
# for 25 topics it's pretty messy: no consistent theme is evolved overtime (to me), some are present for a couple of months (like dark-pink n13 in May and June)
# to see the topwords for a topic:
topwords %>%
filter(topic == 13)
# for 25 topics it's pretty messy: no consistent theme is evolved overtime (to me), some are present for a couple of months (like dark-pink n13 in May and June)
# to see the topwords for a topic:
top_words %>%
filter(topic == 13)
save(file = "model_25t.Rda", c(month_model, beta, gamma))
save("model_25t.Rda", c(month_model, beta, gamma))
save("model_25t.Rda", month_model, beta, gamma)
save(month_model, beta, gamma, file = "model_25t.rda")
# save(month_model, beta, gamma, file = "model_25t.rda")
load("model_25t.rda")
month_model <- LDA(dtm, k = 10, control = list(seed = 1234))
month_model
# beta -- probabilities of words in topics (sorted, top 10 words selected)
beta <- tidy(month_model, matrix = "beta")
top_words <- beta %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_words)
top_words %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
# gamma: probability of topics in documents (months in our case)
gamma <- tidy(month_model, matrix = "gamma")
# distribution of topics over time
gamma %>%
group_by(document, topic) %>%
summarise(avrg = mean(gamma)) %>%
separate(document, into = c("n", "month"), sep = "_") %>%
mutate(n = as.numeric(n)) %>%
filter(n != 1) %>%
ggplot(aes(x = as.factor(n), y = avrg, fill = as.factor(topic))) + geom_col() +
scale_fill_manual(values = as.vector(alphabet(n = 25))) +
labs(x = "Month",
y = "Distribution of topics",
fill = "Number of topic")
# a model with 10 topics shows similar homogeneous thematics for Feb, but results technically seems more relatable (some topics are distributed in more than 2 months, like the 8th)
top_words %>%
filter(topic == 8)
top_words <- beta %>%
group_by(topic) %>%
slice_max(beta, n = 20) %>%
ungroup() %>%
arrange(topic, -beta)
# a model with 10 topics shows similar homogeneous thematics for Feb, but results technically seems more relatable (some topics are distributed in more than 2 months, like the 8th)
top_words %>%
filter(topic == 8)
save(month_model, beta, gamma, file = "model_10t.rda")
?unnest_tokens
month_model
top_words <- beta %>%
group_by(topic) %>%
slice_max(beta, n = 20) %>%
ungroup() %>%
arrange(topic, -beta)
write.csv(top_words, "beta_10t.csv")
# OR
load("model_25t.rda") # will load model with 25 topics
month_model
# beta -- probabilities of words in topics (sorted, top 10 words selected)
beta <- tidy(month_model, matrix = "beta")
top_words <- beta %>%
group_by(topic) %>%
slice_max(beta, n = 20) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_words)
write.csv(top_words, "beta_25t.csv")
